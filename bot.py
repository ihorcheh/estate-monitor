import requests
from bs4 import BeautifulSoup
import time
import os

# –¢–≤–æ—ó –¥–∞–Ω—ñ –∑ —Å–µ–∫—Ä–µ—Ç—ñ–≤ GitHub
TOKEN = os.getenv('TELEGRAM_TOKEN')
CHAT_ID = os.getenv('CHAT_ID')

# –°–ø–∏—Å–æ–∫ —Ç–≤–æ—ó—Ö –ø–æ—Å–∏–ª–∞–Ω—å
URLS = [
    "https://www.x-estate.com/offers?city=5c87a27fe758c42fbc38bca3&category=5cf3da6afe460b4aa52ab4b9&sort=date&order=desc&min_price=1000&max_price=15000",
    "https://dom.ria.com/uk/search?category=1&realty_type=2&operation=1&state_id=7&city_ids=7&price_cur=1&sort=created_at",
    "https://rem.ua/ua/search?type=apartments&city=kharkov&priceMin=1000&priceMax=15000&currency=1",
    "https://lun.ua/sale/kharkiv/flats?price_min=100000&price_max=700000&currency=UAH&sort=insert_time",
    "https://kn.ua/ua/flats/sale/?city=1&price2%5Bfrom%5D=1000&price2%5Bto%5D=15000&sort=date",
    "https://rieltor.ua/harkov/flats-sale/?price_min=100000&price_max=650000&sort=bycreated",
    "https://valion.estate/uk/flats/search"
]

# –§–∞–π–ª –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–Ω–∞–π–¥–µ–Ω–∏—Ö ID –æ–≥–æ–ª–æ—à–µ–Ω—å, —â–æ–± –Ω–µ —Å–ø–∞–º–∏—Ç–∏ –¥—É–±–ª—è–º–∏
DB_FILE = "seen_ids.txt"

def send_msg(text):
    url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
    params = {"chat_id": CHAT_ID, "text": text, "parse_mode": "HTML"}
    requests.get(url, params=params)

def get_seen_ids():
    if not os.path.exists(DB_FILE):
        return set()
    with open(DB_FILE, 'r') as f:
        return set(line.strip() for line in f)

def save_id(realty_id):
    with open(DB_FILE, 'a') as f:
        f.write(f"{realty_id}\n")

def check_sites():
    seen_ids = get_seen_ids()
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

    for url in URLS:
        try:
            print(f"–ü–µ—Ä–µ–≤—ñ—Ä—è—é: {url.split('/')[2]}") # –î—Ä—É–∫—É—î –Ω–∞–∑–≤—É —Å–∞–π—Ç—É –≤ –ª–æ–≥
            response = requests.get(url, headers=headers, timeout=15)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –®—É–∫–∞—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è (—Å–ø—Ä–æ—â–µ–Ω–∏–π –ø–æ—à—É–∫ –¥–ª—è –≤—Å—ñ—Ö —Å–∞–π—Ç—ñ–≤)
            links = []
            for a in soup.find_all('a', href=True):
                href = a['href']
                # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø–æ—Å–∏–ª–∞–Ω–Ω—è, —Å—Ö–æ–∂—ñ –Ω–∞ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è
                if any(x in href for x in ['offer', 'realty', 'flat', 'apartments', 'uk/flats']):
                    if href.startswith('/'):
                        # –î–æ–¥–∞—î–º–æ –¥–æ–º–µ–Ω, —è–∫—â–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –≤—ñ–¥–Ω–æ—Å–Ω–µ
                        domain = url.split('/')[0] + "//" + url.split('/')[2]
                        href = domain + href
                    if href not in seen_ids and url.split('/')[2] in href:
                        links.append(href)
            
            # –ë–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ –ø–µ—Ä—à—ñ 2 –Ω–æ–≤—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∑ –∫–æ–∂–Ω–æ–≥–æ —Å–∞–π—Ç—É, —â–æ–± –Ω–µ –∑–∞–±–∞–Ω–∏–ª–∏
            for link in links[:2]:
                send_msg(f"üè† <b>–ù–æ–≤–∏–π –æ–±'—î–∫—Ç!</b>\n\n–î–∂–µ—Ä–µ–ª–æ: {url.split('/')[2]}\n–ü–æ–∫–ª–∏–∫–∞–Ω–Ω—è: {link}")
                save_id(link)
                seen_ids.add(link)
                time.sleep(2) # –ü–∞—É–∑–∞ –º—ñ–∂ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è–º–∏

        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤—ñ—Ä—Ü—ñ {url}: {e}")

if __name__ == "__main__":
    check_sites()
