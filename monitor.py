import cloudscraper
from bs4 import BeautifulSoup
import time
import os
import random

# –î–∞–Ω—ñ –∑ GitHub Secrets
TOKEN = os.getenv('TELEGRAM_TOKEN')
CHAT_ID = os.getenv('CHAT_ID')

URLS = [
    "https://www.x-estate.com/offers?city=5c87a27fe758c42fbc38bca3&category=5cf3da6afe460b4aa52ab4b9&sort=date&order=desc&min_price=1000&max_price=15000",
    "https://dom.ria.com/uk/search?category=1&realty_type=2&operation=1&state_id=7&city_ids=7&price_cur=1&sort=created_at",
    "https://rem.ua/ua/search?type=apartments&city=kharkov&priceMin=1000&priceMax=15000&currency=1",
    "https://lun.ua/sale/kharkiv/flats?price_min=100000&price_max=700000&currency=UAH&sort=insert_time",
    "https://kn.ua/ua/flats/sale/?city=1&price2%5Bfrom%5D=1000&price2%5Bto%5D=15000&sort=date",
    "https://rieltor.ua/harkov/flats-sale/?price_min=100000&price_max=650000&sort=bycreated",
    "https://valion.estate/uk/flats/search"
]

DB_FILE = "seen_ids.txt"

def send_msg(text):
    url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
    params = {"chat_id": CHAT_ID, "text": text, "parse_mode": "HTML"}
    try:
        requests.post(url, params=params)
    except:
        pass

def get_seen_ids():
    if not os.path.exists(DB_FILE):
        return set()
    with open(DB_FILE, 'r') as f:
        return set(line.strip() for line in f)

def save_id(realty_id):
    with open(DB_FILE, 'a') as f:
        f.write(f"{realty_id}\n")

def check_sites():
    seen_ids = get_seen_ids()
    # –°—Ç–≤–æ—Ä—é—î–º–æ "—Ä–æ–∑—É–º–Ω–∏–π" —Å–∫–∞–Ω–µ—Ä, —è–∫–∏–π –æ–±—Ö–æ–¥–∏—Ç—å –∑–∞—Ö–∏—Å—Ç
    scraper = cloudscraper.create_scraper(
        browser={
            'browser': 'chrome',
            'platform': 'windows',
            'desktop': True
        }
    )

    for url in URLS:
        try:
            site_name = url.split('/')[2]
            print(f"–°–∫–∞–Ω—É–≤–∞–Ω–Ω—è: {site_name}")
            
            # –í–∏–ø–∞–¥–∫–æ–≤–∞ –∑–∞—Ç—Ä–∏–º–∫–∞ –ø–µ—Ä–µ–¥ –∫–æ–∂–Ω–∏–º —Å–∞–π—Ç–æ–º, —â–æ–± –Ω–µ —Å–ø–∞–ª–∏—Ç–∏—Å—å
            time.sleep(random.uniform(3, 7))
            
            response = scraper.get(url, timeout=20)
            if response.status_code != 200:
                print(f"–°–∞–π—Ç {site_name} –ø–æ–≤–µ—Ä–Ω—É–≤ –ø–æ–º–∏–ª–∫—É {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            links = []

            # –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –ø–æ—à—É–∫ –ø–æ—Å–∏–ª–∞–Ω—å
            for a in soup.find_all('a', href=True):
                href = a['href']
                # –°–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —Ñ—ñ–ª—å—Ç—Ä–∏ –¥–ª—è –Ω–µ—Ä—É—Ö–æ–º–æ—Å—Ç—ñ
                is_realty = any(x in href.lower() for x in ['offer', 'realty', 'flat', 'apartments', 'uk/flats', 'sale/'])
                
                if is_realty:
                    if href.startswith('/'):
                        domain = url.split('/')[0] + "//" + url.split('/')[2]
                        href = domain + href
                    
                    if href not in seen_ids and site_name in href:
                        links.append(href)

            # –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ —Ç—ñ–ª—å–∫–∏ –Ω–∞–π—Å–≤—ñ–∂—ñ—à—ñ (–ø–µ—Ä—à—ñ 3)
            for link in list(dict.fromkeys(links))[:3]:
                send_msg(f"‚ú® <b>–ó–Ω–∞–π–¥–µ–Ω–æ –æ–±'—î–∫—Ç!</b>\n\nüìç –î–∂–µ—Ä–µ–ª–æ: {site_name}\nüîó <a href='{link}'>–í—ñ–¥–∫—Ä–∏—Ç–∏ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è</a>")
                save_id(link)
                seen_ids.add(link)
                time.sleep(1)

        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ {url}: {e}")

if __name__ == "__main__":
    check_sites()
